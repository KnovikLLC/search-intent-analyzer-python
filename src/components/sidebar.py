# sidebar.py ‚Äî Sidebar Configuration Component

import streamlit as st
from src.config.constants import __version__, AUTHOR, AVAILABLE_MODELS


def render_sidebar():
    """Render sidebar with configuration options."""
    st.header("‚öôÔ∏è Settings")
    st.caption(f"Version: {__version__} ‚Ä¢ Author: {AUTHOR}")

    st.subheader("ü§ñ LLM Configuration")

    selected_model = st.selectbox(
        "Select Ollama Model",
        AVAILABLE_MODELS,
        index=0,
        help="Recommended: llama3.2:3b for best balance of speed and accuracy",
    )

    ollama_url = st.text_input(
        "Ollama API URL",
        "http://localhost:11434",
        help="Default: http://localhost:11434",
    )

    st.divider()

    _render_help_sections()

    return selected_model, ollama_url


def _render_help_sections():
    """Render help and about sections."""
    with st.expander("üìñ Setup Instructions"):
        st.markdown(
            """
        **1. Install Ollama:**
        ```bash
        brew install ollama
        ```
        
        **2. Start Ollama Server:**
        ```bash
        ollama serve
        ```
        
        **3. Pull Model (in new terminal):**
        ```bash
        ollama pull llama3.2:3b
        ```
        
        **4. Verify Installation:**
        ```bash
        ollama list
        ```
        
        **Note:** First run may take time to download the model (~2GB)
        """
        )

    with st.expander("‚ÑπÔ∏è About This Tool"):
        st.markdown(
            """
        **LLM-Powered Intent Analysis:**
        - ‚úÖ No API keys required
        - ‚úÖ 100% local & private
        - ‚úÖ Fast & accurate
        - ‚úÖ No Firecrawl needed
        
        **How it works:**
        1. Enter search queries
        2. LLM analyzes user intent
        3. Get detailed results with reasoning
        """
        )
